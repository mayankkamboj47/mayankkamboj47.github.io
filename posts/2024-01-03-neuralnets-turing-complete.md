---
layout : post
title  : Another proof of neural nets' Turing Completeness
---

We can prove Turing completeness of neural networks by implementing a Turing-complete model in them. In this post, we implement the substitution model of computation, a single-instruction model of computation I describe [here](/posts/2023-05-12-a-substitution-model-of-computation), using a neural net.

The proof is simple – make a neural network that acts like the model’s instruction `sub s1 s2` (the only instruction in the model) where `s1` and `s2` are strings of symbols. We begin with neurons that can execute functions of the sort $if$ $a_1==b_1$ and $a_2==b_2$ and $…$ and $a_n=b_n$ then $c$ else $d$, where $a_1…a_n$, $b_1 … b_n$, $c$ and $d$ are either whole number inputs to the neuron or hardcoded literals. A section at the end of the post shows how to build these from scratch from regular neurons. Then, we take the symbols of the substitution model, of which there are countably infinite, and convert each into a unique natural number to each of them so that we can feed it to the neuralnet. To implement the instruction `sub s1 s2`, which can be read as “substitute the string of symbols `s2` for `s1` in the input”, we note that a single neuron is enough to handle cases where `s1`, `s2` and the input string are each of length 1. For other cases, we go from left to right in the input string, examining each substring for whether it is equal to `s1`, and substituting. One layer will be responsible for examining one such substring, and the network will be $n$ layers deep overall ($n\mid s \mid$ really, the reason for which will become clear once I demonstrate how the neuralnet is laid out). We can ignore having an exact match between layers and substrings, as long as there are enough layers; as you’ll see, the extra layers have no effect. 
      
      
      Since neuralnets layers expect fixed input sizes and the substitution model doesn’t, we’ll also need to adjust the input size and layer heights to match. For this, we resize the neural net to match the size of the input. This resizing process could be carried out by an algorithm – it doesn’t require any creativity. In this sense, it’s still the same neuralnet, or architecture, just adjusted to fit a given input size. The size of each further layer is set so that it could store the output if a successful substitution were to happen for the substring it is examining. If no substitution happens, or s2 is smaller than s1, the empty neurons just store the special sentinel 1 (this is why we reserved the odd numbers), and pass them along. 
    Here is how it all looks combined, in the case with len(s2)=2, and len(s1)=1. 

![Neuralnet](/img/graph.svg)  

Here, we have the first 5 layers of the  16-layer neuralnet for the case where s1 is a single symbol, s2 is of length 2, and the input, of course matching the depth of the network, is of size 8. Each layer is examining a single symbol (the one from which black lines are coming out). Depending on the value of this symbol, the neurons either emit the value directly in front of them (the pink connection), or the value that would come in front after the symbol being examined is substituted with s2 (the blue connection). The neurons which only have a blue connection emit the sentinel 1, if there is no substitution. Neurons with only the pink connection simply forward their values. 
If s1 were larger, say of size n, then we would examine n neurons at a time (the black lines would emanate from n neurons, connecting each to the entirely of the next layer), and it would be based on the value of these n neurons together, that one value or another would be emitted by the examining neuron.


In cases where s2 is shorter than s1, we don’t go decreasing the layer size as we proceed deeper. We just use the sentinel 1 in the positions left empty at the end after substitution, and keep the layers the same height. 


A final point of subtlety, you may have noted, arises when s2 contains part of the substring s1. For example, say the instruction were, after numericalisation of tokens, sub 4 64. Now, after the substitution, the next layer ends up examining the 4 in 64 and will carry out the substitution again, resulting in disaster. Instead, we’d want to skip the 64 entirely, and go to the next symbol. However, it would make the structure of each layer’s connections depend on the substitutions, so we instead ensure that for each symbol, we don’t directly substitute its symbol value, but substitute one less than that symbol value. Thus we wouldn’t substitute 64 in 4’s place, but 53, where each digit is decreased by one. Finally, because of this subtlety, to go all the way through in examining the input, we need not n layers, but n*len(s2) layers. 


And that finishes the design of the network that can execute the sub s1 s2 instruction in general. You may have noticed we still need to handle the special symbols. But we don’t need to, it is better to modify the substitution model itself. In the modified model, the input, output and intermediate strings must start with the symbol ^ and end with dollar sign $$$ - which makes the instructions with these symbols just as if they were ordinary symbols. Also, instructions having these symbols must have them in both s1 and s2. That is, sub $$$ abc is invalid, but sub $$$ abc$$$ is valid as a way of appending the string “abc” to the end of the input. Now, since $$$ and ^ are just like any other ordinary symbol, they are handled by the above general implementation itself.


I also want to say some things about the size of the input. A layer needs to be at least as tall as its input, but it could be taller – since the layers can just emit and accept sentinels in this case. Therefore, given instructions in order, each neural net could keep in mind the maximal input size it could get, and have its input layer be that tall. This maximal size S for a layer is decided by the equation $S(l_n) = S(l_n-1)*len(s2)$ and $S(l_0)=len(input)$. This means that given a row of sub instructions, it is possible to make networks for each of the instructions and join them into one big network. But since Turing completeness is only achieved once we can loop through this row of instructions over and over again, or in our case, run the output through the neuralnet over and over until it stops changing (aka. we get a fixed point), we may have to resize the network per iteration While that may be a point of inconvenience and go against the elegance of the Turing-completeness of this architecture, the resizing could be done by an algorithm and doesn’t require any creativity whatsoever. On the other hand, if one is okay with networks that are infinitely deep, then it is possible to just pretend that the sequence of instructions is infinitely repeating and design an infinitely deep network that is sure to converge for any computable problem. 

## Implementing the neurons

Let’s now implement the neurons, as I promised. We’ll use simple ReLU activation neurons for this, which we will assemble together in layers. Another way to think of it is that we’re breaking down a single layer of the networks used in the proof into multiple, simpler layers.
To implement a single neuron, take two ReLU neurons, one detecting whether the input is >=C and another whether it is <=C. In the next layer, have a neuron add these together and set its bias such that it is only activated when both of its inputs are. This neuron now only activates when the initial input is exactly C. One could make multiple such neurons, and since each of their calculations can be done in parallel, they stack up nicely in a single layer. Finally, to implement “and”, we could have a neuron afterwards add all of these up, and again have the bias value such that the neuron only activates if all its inputs are activated. Now, based on the activation of this neuron, we could output either C or D conditionally. 